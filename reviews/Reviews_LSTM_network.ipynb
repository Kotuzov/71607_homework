{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=5000)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('reviews.txt', sep=\"\\t\", header=None)\n",
    "\n",
    "data.columns = ['review', 'degree']\n",
    "\n",
    "\n",
    "reviews = data['review'].copy()\n",
    "degree = data.review.apply(lambda s : s[0])\n",
    "\n",
    "data['review'] = reviews.copy()\n",
    "data['degree'] = degree.copy()\n",
    "\n",
    "data['review'] = data.review.apply(lambda s: ' '.join(nltk.word_tokenize(s))[1:])\n",
    "\n",
    "#del reviews\n",
    "#del degree\n",
    "data.head(5)\n",
    "#рейтинг\n",
    "#0 \tnegative\n",
    "#1 \tsomewhat negative\n",
    "#2 \tneutral\n",
    "#3 \tsomewhat positive\n",
    "#4 \tpositive\n",
    "\n",
    "data['negative'] = (data.degree == 0).astype(float)\n",
    "data['somewhat negative'] = (data.degree == '1').astype(float)\n",
    "data['neutral'] = (data.degree == '2').astype(float)\n",
    "data['somewhat positive'] = (data.degree == '3').astype(float)\n",
    "data['positive'] = (data.degree == '4').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "cv.fit(data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15164"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = len(cv.vocabulary_)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11783, 9184, 4554, 3473, 13440, 287, 13438, 14798, 7182, 5793, 5298, 13440, 5809, 7182, 526, 5793, 5298, 13440, 5568, 12367, 9184, 14815, 9161, 598, 1868, 9042, 9184, 14815, 589, 13616, 8765, 9184, 12799], [10535, 7110, 620, 4477, 6809, 7182, 15021, 11696], [4918, 9184, 14993, 13129, 15025, 6182, 6128, 13579, 12103, 13533, 13491, 9228], [10073, 13526, 2559, 9184, 4598, 620, 503, 13440, 7100, 1340, 3338, 620, 8789, 9184, 13732, 9282, 7364, 12315, 9252], [620, 8174, 14840], [9184, 8918, 4511, 10355, 11294, 6754, 12075, 9712, 1883, 13440, 13609, 2165, 14073, 8469, 3096], [7182, 9947, 8427], [9184, 12313, 13475, 14934, 14932, 7194, 9923, 7827, 10685, 5462], [7194, 3901, 7714, 15109, 14934, 8765], [2965, 6168, 7194, 5298, 13440, 11441, 10709]]\n"
     ]
    }
   ],
   "source": [
    "sent_to_vec = []\n",
    "for s in data['review']:\n",
    "    sent_l = nltk.word_tokenize(s)\n",
    "    new_sent = []\n",
    "    for w in sent_l:\n",
    "        try:\n",
    "            new_sent.append(cv.vocabulary_[w])\n",
    "        except: pass\n",
    "    sent_to_vec.append(new_sent)\n",
    "     \n",
    "print(sent_to_vec[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [11783, 9184, 4554, 3473, 13440, 287, 13438, 1...\n",
       "1    [10535, 7110, 620, 4477, 6809, 7182, 15021, 11...\n",
       "2    [4918, 9184, 14993, 13129, 15025, 6182, 6128, ...\n",
       "3    [10073, 13526, 2559, 9184, 4598, 620, 503, 134...\n",
       "4                                   [620, 8174, 14840]\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'] = sent_to_vec\n",
    "data.review.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_review_length = data.review.str.len().max()\n",
    "max_review_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>somewhat negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>somewhat positive</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative  somewhat negative  neutral  somewhat positive  positive\n",
       "0       0.0                1.0      0.0                0.0       0.0\n",
       "1       0.0                0.0      0.0                0.0       1.0\n",
       "2       0.0                1.0      0.0                0.0       0.0\n",
       "3       0.0                0.0      0.0                1.0       0.0\n",
       "4       0.0                1.0      0.0                0.0       0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.review\n",
    "y = data.drop(['review', 'degree'],axis = 1)\n",
    "y.head(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=36)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(7)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, dtype='int32')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6396, 43)\n",
      "(2133, 43)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(20, kernel_regularizer=<keras.reg..., recurrent_regularizer=<keras.reg...)`\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 43, 32)            485248    \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 43, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_63 (LSTM)               (None, 20)                4240      \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 489,593\n",
      "Trainable params: 489,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/55\n",
      "6396/6396 [==============================] - 9s 1ms/step - loss: 1.5734 - acc: 0.2483\n",
      "Epoch 2/55\n",
      "6396/6396 [==============================] - 4s 645us/step - loss: 1.3079 - acc: 0.2642\n",
      "Epoch 3/55\n",
      "6396/6396 [==============================] - 4s 647us/step - loss: 1.2454 - acc: 0.2794\n",
      "Epoch 4/55\n",
      "6396/6396 [==============================] - 4s 651us/step - loss: 1.1771 - acc: 0.3374\n",
      "Epoch 5/55\n",
      "6396/6396 [==============================] - 4s 583us/step - loss: 1.0640 - acc: 0.4146\n",
      "Epoch 6/55\n",
      "6396/6396 [==============================] - 4s 571us/step - loss: 0.9524 - acc: 0.4636\n",
      "Epoch 7/55\n",
      "6396/6396 [==============================] - 4s 579us/step - loss: 0.8808 - acc: 0.4819\n",
      "Epoch 8/55\n",
      "6396/6396 [==============================] - 4s 581us/step - loss: 0.8226 - acc: 0.5159\n",
      "Epoch 9/55\n",
      "6396/6396 [==============================] - 4s 586us/step - loss: 0.7830 - acc: 0.5433\n",
      "Epoch 10/55\n",
      "6396/6396 [==============================] - 4s 582us/step - loss: 0.7539 - acc: 0.5721\n",
      "Epoch 11/55\n",
      "6396/6396 [==============================] - 4s 582us/step - loss: 0.7274 - acc: 0.5833\n",
      "Epoch 12/55\n",
      "6396/6396 [==============================] - 4s 580us/step - loss: 0.6998 - acc: 0.6026\n",
      "Epoch 13/55\n",
      "6396/6396 [==============================] - 4s 619us/step - loss: 0.6744 - acc: 0.6243\n",
      "Epoch 14/55\n",
      "6396/6396 [==============================] - 4s 600us/step - loss: 0.6541 - acc: 0.6395\n",
      "Epoch 15/55\n",
      "6396/6396 [==============================] - 4s 589us/step - loss: 0.6109 - acc: 0.6565\n",
      "Epoch 16/55\n",
      "6396/6396 [==============================] - 4s 567us/step - loss: 0.5908 - acc: 0.6689\n",
      "Epoch 17/55\n",
      "6396/6396 [==============================] - 4s 578us/step - loss: 0.5515 - acc: 0.6862\n",
      "Epoch 18/55\n",
      "6396/6396 [==============================] - 4s 586us/step - loss: 0.5412 - acc: 0.7008\n",
      "Epoch 19/55\n",
      "6396/6396 [==============================] - 4s 557us/step - loss: 0.5189 - acc: 0.7111\n",
      "Epoch 20/55\n",
      "6396/6396 [==============================] - 3s 527us/step - loss: 0.4900 - acc: 0.7195\n",
      "Epoch 21/55\n",
      "6396/6396 [==============================] - 3s 529us/step - loss: 0.4705 - acc: 0.7311\n",
      "Epoch 22/55\n",
      "6396/6396 [==============================] - 3s 537us/step - loss: 0.4682 - acc: 0.7256\n",
      "Epoch 23/55\n",
      "6396/6396 [==============================] - 3s 537us/step - loss: 0.4398 - acc: 0.7427\n",
      "Epoch 24/55\n",
      "6396/6396 [==============================] - 3s 539us/step - loss: 0.4319 - acc: 0.7427\n",
      "Epoch 25/55\n",
      "6396/6396 [==============================] - 4s 549us/step - loss: 0.4064 - acc: 0.7552\n",
      "Epoch 26/55\n",
      "6396/6396 [==============================] - 4s 568us/step - loss: 0.3986 - acc: 0.7570\n",
      "Epoch 27/55\n",
      "6396/6396 [==============================] - 4s 610us/step - loss: 0.3954 - acc: 0.7583\n",
      "Epoch 28/55\n",
      "6396/6396 [==============================] - 4s 613us/step - loss: 0.3729 - acc: 0.7683\n",
      "Epoch 29/55\n",
      "6396/6396 [==============================] - 4s 587us/step - loss: 0.3732 - acc: 0.7672\n",
      "Epoch 30/55\n",
      "6396/6396 [==============================] - 4s 577us/step - loss: 0.3468 - acc: 0.7789\n",
      "Epoch 31/55\n",
      "6396/6396 [==============================] - 4s 596us/step - loss: 0.3448 - acc: 0.7770\n",
      "Epoch 32/55\n",
      "6396/6396 [==============================] - 4s 614us/step - loss: 0.3279 - acc: 0.7860\n",
      "Epoch 33/55\n",
      "6396/6396 [==============================] - 4s 656us/step - loss: 0.3232 - acc: 0.7847\n",
      "Epoch 34/55\n",
      "6396/6396 [==============================] - 4s 635us/step - loss: 0.3305 - acc: 0.7778\n",
      "Epoch 35/55\n",
      "6396/6396 [==============================] - 5s 710us/step - loss: 0.3146 - acc: 0.7892\n",
      "Epoch 36/55\n",
      "6396/6396 [==============================] - 4s 651us/step - loss: 0.3021 - acc: 0.7952\n",
      "Epoch 37/55\n",
      "6396/6396 [==============================] - 4s 632us/step - loss: 0.3002 - acc: 0.7971\n",
      "Epoch 38/55\n",
      "6396/6396 [==============================] - 4s 655us/step - loss: 0.2935 - acc: 0.7917\n",
      "Epoch 39/55\n",
      "6396/6396 [==============================] - 4s 681us/step - loss: 0.2877 - acc: 0.7978\n",
      "Epoch 40/55\n",
      "6396/6396 [==============================] - 4s 683us/step - loss: 0.2933 - acc: 0.7960\n",
      "Epoch 41/55\n",
      "6396/6396 [==============================] - 4s 587us/step - loss: 0.2784 - acc: 0.8033\n",
      "Epoch 42/55\n",
      "6396/6396 [==============================] - 3s 508us/step - loss: 0.2600 - acc: 0.8078\n",
      "Epoch 43/55\n",
      "6396/6396 [==============================] - 3s 506us/step - loss: 0.2618 - acc: 0.8071 0s - loss: 0.2596 - acc\n",
      "Epoch 44/55\n",
      "6396/6396 [==============================] - 3s 507us/step - loss: 0.2607 - acc: 0.8077\n",
      "Epoch 45/55\n",
      "6396/6396 [==============================] - 3s 507us/step - loss: 0.2607 - acc: 0.8060\n",
      "Epoch 46/55\n",
      "6396/6396 [==============================] - 3s 512us/step - loss: 0.3028 - acc: 0.7878\n",
      "Epoch 47/55\n",
      "6396/6396 [==============================] - 3s 510us/step - loss: 0.2496 - acc: 0.8102 0s - loss: 0.2484 - acc:\n",
      "Epoch 48/55\n",
      "6396/6396 [==============================] - 3s 520us/step - loss: 0.2460 - acc: 0.8097\n",
      "Epoch 49/55\n",
      "6396/6396 [==============================] - 3s 502us/step - loss: 0.2527 - acc: 0.8075 1\n",
      "Epoch 50/55\n",
      "6396/6396 [==============================] - 3s 466us/step - loss: 0.2310 - acc: 0.8139\n",
      "Epoch 51/55\n",
      "6396/6396 [==============================] - 3s 465us/step - loss: 0.2281 - acc: 0.8138\n",
      "Epoch 52/55\n",
      "6396/6396 [==============================] - 3s 469us/step - loss: 0.2336 - acc: 0.8138\n",
      "Epoch 53/55\n",
      "6396/6396 [==============================] - 3s 495us/step - loss: 0.2534 - acc: 0.8061\n",
      "Epoch 54/55\n",
      "6396/6396 [==============================] - 3s 469us/step - loss: 0.2366 - acc: 0.8138\n",
      "Epoch 55/55\n",
      "6396/6396 [==============================] - 3s 487us/step - loss: 0.2364 - acc: 0.8125\n",
      "Accuracy: 31.83%\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "num_classes = 5\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(20, W_regularizer=l2(0.01), U_regularizer = l2(0.001)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, nb_epoch=55, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
